{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# multiprocessing 이용해 좀 더 빠르게 크롤링하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naver blog _ title/ date/ contents / id / comments 수집\r\n",
    "## 홈트레이닝, 홈트 관련 트랜드 변화 확인 \r\n",
    "광고 데이터 제거 : 홈트 | 홈트레이닝 -업체 -제공받 -지원받 -원고료 -협찬 -대가 -소정의  -서포터즈 -광고 -제작비"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import requests\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import time\r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.common.keys import Keys\r\n",
    "\r\n",
    "import urllib.request\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "\r\n",
    "from multiprocessing import Pool # Pool import하기"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import multiprocessing\r\n",
    "multiprocessing.cpu_count() # 6코어 12processor 확인"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# second 동안 스크롤다운 함수 정의\r\n",
    "import datetime\r\n",
    "\r\n",
    "def doScrollDown(whileSeconds):\r\n",
    "    start = datetime.datetime.now()\r\n",
    "    end = start + datetime.timedelta(seconds=whileSeconds)\r\n",
    "    while True:\r\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\r\n",
    "        time.sleep(1)\r\n",
    "        if datetime.datetime.now() > end:\r\n",
    "            break\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_Data(driver):\r\n",
    "    driver.switch_to.frame('cafe_main') # 프레임 변경\r\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser') # 페이지 준비\r\n",
    "\r\n",
    "    #제목/ 닉네임/ 날짜 \r\n",
    "    title = soup.select('h3.title_text')[0].text.strip() #h3태그에 title_text클래스\r\n",
    "    nickname = soup.select('a.nickname')[0].text.strip() #a테그에 nickname클래스\r\n",
    "    date = soup.select('span.date')[0].text[:10] #시간제외 날짜까지만 인덱싱하여 가져옴\r\n",
    "    \r\n",
    "    #내용\r\n",
    "    try:\r\n",
    "        content = soup.select('div.ContentRenderer')[0].text #div태그에 ContentRenderer클래스\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "    # 댓글 : 여러개 존재하므로 모두가져온다음 text만 뽑음 \r\n",
    "    comments_all = soup.select('span.text_comment')\r\n",
    "    comment_list =[]\r\n",
    "    for comment in comments_all:\r\n",
    "        comment = comment.text.strip()\r\n",
    "        comment_list.append(comment)\r\n",
    "\r\n",
    "    # 댓글 : 리스트-> 문자열로 저장\r\n",
    "    seperator = '||'\r\n",
    "    comments = seperator.join(comment_list)\r\n",
    "\r\n",
    "    # 제목/ 닉네임/ 날짜 / 내용/ 댓글 데이터-> 리스트로 저장\r\n",
    "    post = [title , nickname, date, content,comments]\r\n",
    "\r\n",
    "    return post"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "import time\r\n",
    "from multiprocessing import Pool, Manager\r\n",
    "search_word = \"삼성\" #검색어 지정\r\n",
    "end = 300 #마지막 뉴스 지정\r\n",
    " \r\n",
    "#list를 공유 하기 위해\r\n",
    "manager = Manager()\r\n",
    "title_list = manager.list()\r\n",
    " \r\n",
    "def title_to_list(start):\r\n",
    "    global title_list\r\n",
    "    #url making\r\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}&start={}'.format(search_word,start)\r\n",
    "    req = requests.get(url)\r\n",
    " \r\n",
    "    #정상적인 request 확인\r\n",
    "    if req.ok:\r\n",
    "        html = req.text\r\n",
    "        soup = BeautifulSoup(html,'html.parser')\r\n",
    "    \r\n",
    "        #뉴스제목 뽑아오기\r\n",
    "        titles = soup.select(\r\n",
    "            'ul.type01 > li > dl > dt > a'\r\n",
    "        )\r\n",
    " \r\n",
    "        #list에 넣어준다\r\n",
    "        for title in titles:\r\n",
    "            title_list.append(title['title'])\r\n",
    " \r\n",
    "if __name__ == '__main__':\r\n",
    "    start_time = time.time()\r\n",
    "    pool = Pool(processes=12) #4개의 프로세스 동시에 작동\r\n",
    "    pool.map(title_to_list,range(1,end,10)) #title_to_list라는 함수에 1 ~ end까지 10씩늘려가며 인자로 적용\r\n",
    "    print(title_list)\r\n",
    "    print(\"실행 시간 : %s초\" % (time.time() - start_time))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "import time\r\n",
    "search_word = \"삼성\" #검색어 지정\r\n",
    "start = 1\r\n",
    "end = 300 #마지막 뉴스 지정\r\n",
    "title_list = []\r\n",
    " \r\n",
    "if __name__ == '__main__':\r\n",
    "    start_time = time.time()\r\n",
    "    while 1:\r\n",
    "        if start > end:\r\n",
    "            break\r\n",
    "        print(start)\r\n",
    " \r\n",
    "        url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}&start={}'.format(search_word,start)\r\n",
    "        req = requests.get(url)\r\n",
    " \r\n",
    "        #정상적인 request 확인\r\n",
    "        if req.ok:\r\n",
    "            html = req.text\r\n",
    "            soup = BeautifulSoup(html,'html.parser')\r\n",
    "    \r\n",
    "         #뉴스제목 뽑아오기\r\n",
    "            titles = soup.select(\r\n",
    "                'ul.type01 > li > dl > dt > a'\r\n",
    "            )\r\n",
    " \r\n",
    "            #list에 넣어준다\r\n",
    "            for title in titles:\r\n",
    "                title_list.append(title['title'])\r\n",
    "        start += 10\r\n",
    "    print(title_list)\r\n",
    "    print(\"실행 시간 : %s초\" % (time.time() - start_time))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "61\n",
      "71\n",
      "81\n",
      "91\n",
      "101\n",
      "111\n",
      "121\n",
      "131\n",
      "141\n",
      "151\n",
      "161\n",
      "171\n",
      "181\n",
      "191\n",
      "201\n",
      "211\n",
      "221\n",
      "231\n",
      "241\n",
      "251\n",
      "261\n",
      "271\n",
      "281\n",
      "291\n",
      "[]\n",
      "실행 시간 : 9.767016410827637초\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "import time\r\n",
    "from multiprocessing import Pool, Manager\r\n",
    "search_word = \"삼성\" #검색어 지정\r\n",
    "end = 10 #마지막 뉴스 지정\r\n",
    " \r\n",
    "#list를 공유 하기 위해\r\n",
    "manager = Manager()\r\n",
    "title_list = manager.list()\r\n",
    " \r\n",
    "def title_to_list(start):\r\n",
    "    global title_list\r\n",
    "    #url making\r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%ED%99%88%ED%8A%B8%20%7C%20%ED%99%88%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%8B%9D%20-%EC%97%85%EC%B2%B4%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%20-%EC%A7%80%EC%9B%90%EB%B0%9B%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%20-%ED%98%91%EC%B0%AC%20-%EB%8C%80%EA%B0%80%20-%EC%86%8C%EC%A0%95%EC%9D%98%20%20-%EC%84%9C%ED%8F%AC%ED%84%B0%EC%A6%88%20-%EA%B4%91%EA%B3%A0%20-%EC%A0%9C%EC%9E%91%EB%B9%84&ie=utf8&st=rel&date_option=99&date_from=2016.01.01&date_to=2016.01.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20160101to20160131&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "    req = requests.get(url)\r\n",
    " \r\n",
    "    #정상적인 request 확인\r\n",
    "    if req.ok:\r\n",
    "        html = req.text\r\n",
    "        soup = BeautifulSoup(html,'html.parser')\r\n",
    "    \r\n",
    "        #뉴스제목 뽑아오기\r\n",
    "        titles = soup.select(\r\n",
    "            'ul.type01 > li > dl > dt > a'\r\n",
    "        )\r\n",
    " \r\n",
    "        #list에 넣어준다\r\n",
    "        for title in titles:\r\n",
    "            title_list.append(title['title'])\r\n",
    " \r\n",
    "if __name__ == '__main__':\r\n",
    "    start_time = time.time()\r\n",
    "    pool = Pool(processes=12) #4개의 프로세스 동시에 작동\r\n",
    "    pool.map(title_to_list,range(1,end,5)) #title_to_list라는 함수에 1 ~ end까지 10씩늘려가며 인자로 적용\r\n",
    "    print(title_list)\r\n",
    "    print(\"실행 시간 : %s초\" % (time.time() - start_time))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://search.naver.com/search.naver?where=article&query=%ED%99%88%ED%8A%B8%20%7C%20%ED%99%88%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%8B%9D%20-%EC%97%85%EC%B2%B4%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%20-%EC%A7%80%EC%9B%90%EB%B0%9B%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%20-%ED%98%91%EC%B0%AC%20-%EB%8C%80%EA%B0%80%20-%EC%86%8C%EC%A0%95%EC%9D%98%20%20-%EC%84%9C%ED%8F%AC%ED%84%B0%EC%A6%88%20-%EA%B4%91%EA%B3%A0%20-%EC%A0%9C%EC%9E%91%EB%B9%84&ie=utf8&st=rel&date_option=99&date_from=2016.01.01&date_to=2016.01.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20160101to20160131&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "\r\n",
    "pool = Pool(processes=12)\r\n",
    "results = pool.apply_async()\r\n",
    "driver = webdriver.Chrome('./chromedriver') \r\n",
    "driver.get(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from fake_useragent import UserAgent #pip install fake-useragent\r\n",
    "from time import sleep\r\n",
    "from multiprocessing import Pool\r\n",
    "from concurrent.futures import ThreadPoolExecutor\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from selenium import webdriver\r\n",
    "import concurrent.futures\r\n",
    "import urllib.request  \r\n",
    "import requests\r\n",
    "import time\r\n",
    "\r\n",
    "    \r\n",
    "urls = [\"https://namu.wiki/w/%EB%B6%84%EB%A5%98:%EC%A0%95%EC%88%98\", \r\n",
    "        'https://namu.wiki/w/%EB%B6%84%EB%A5%98:%EC%88%98', \r\n",
    "        'https://namu.wiki/w/%EB%B6%84%EB%A5%98:%ED%95%9C%EA%B5%AD%20%EC%95%84%EC%9D%B4%EB%8F%8C', \r\n",
    "        'https://namu.wiki/w/%EB%B6%84%EB%A5%98:%EA%B1%B8%EA%B7%B8%EB%A3%B9']\r\n",
    "\r\n",
    "    \r\n",
    "limit = 20\r\n",
    "\r\n",
    "def get_sublist_href(url: str):\r\n",
    "    namu_link = []\r\n",
    "    request = requests.get(url)\r\n",
    "    sleep(1)\r\n",
    "    \r\n",
    "    parsed_html = BeautifulSoup(request.text, 'html.parser')\r\n",
    "    a_element_tags = parsed_html.find_all('div', attrs={'class' : 'test'})\r\n",
    "    for tag in a_element_tags:\r\n",
    "        for link in tag.find_all('a'):\r\n",
    "            namu_link.append(url + link['href'])\r\n",
    "    \r\n",
    "    namu_link = namu_link[:limit]\r\n",
    "    print('Number of site: ', len(namu_link))\r\n",
    "    return namu_link\r\n",
    "\r\n",
    "def do_html_crawl(url: str):\r\n",
    "    request = requests.get(url)\r\n",
    "    sleep(1)\r\n",
    "    parsed_html = BeautifulSoup(request.text, 'html.parser')\r\n",
    "    return parsed_html\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    start_time = time.time()\r\n",
    "    for url in urls:\r\n",
    "        sub_url_list = get_sublist_href(url)\r\n",
    "        for sub_url in sub_url_list:\r\n",
    "            do_html_crawl(sub_url)\r\n",
    "    print(\"--- elapsed time %s seconds ---\" % (time.time() - start_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of site:  0\n",
      "Number of site:  0\n",
      "Number of site:  0\n",
      "Number of site:  0\n",
      "--- elapsed time 4.351112127304077 seconds ---\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(1,3):  # 3월부터 12월까지\r\n",
    "\r\n",
    "  # 01. 페이지 준비(1월~ 2월 설정)\r\n",
    "  if i == 1:\r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%ED%99%88%ED%8A%B8%20%7C%20%ED%99%88%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%8B%9D%20-%EC%97%85%EC%B2%B4%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%20-%EC%A7%80%EC%9B%90%EB%B0%9B%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%20-%ED%98%91%EC%B0%AC%20-%EB%8C%80%EA%B0%80%20-%EC%86%8C%EC%A0%95%EC%9D%98%20%20-%EC%84%9C%ED%8F%AC%ED%84%B0%EC%A6%88%20-%EA%B4%91%EA%B3%A0%20-%EC%A0%9C%EC%9E%91%EB%B9%84&ie=utf8&st=rel&date_option=99&date_from=2016.01.01&date_to=2016.01.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20160101to20160131&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 2:\r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%ED%99%88%ED%8A%B8%20%7C%20%ED%99%88%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%8B%9D%20-%EC%97%85%EC%B2%B4%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%20-%EC%A7%80%EC%9B%90%EB%B0%9B%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%20-%ED%98%91%EC%B0%AC%20-%EB%8C%80%EA%B0%80%20-%EC%86%8C%EC%A0%95%EC%9D%98%20%20-%EC%84%9C%ED%8F%AC%ED%84%B0%EC%A6%88%20-%EA%B4%91%EA%B3%A0%20-%EC%A0%9C%EC%9E%91%EB%B9%84&ie=utf8&st=rel&date_option=99&date_from=2016.02.01&date_to=2016.02.29&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20160201to20160229&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    " \r\n",
    "  driver = webdriver.Chrome('./chromedriver') \r\n",
    "  #driver.implicitly_wait(4) \r\n",
    "  driver.get(url)\r\n",
    "  time.sleep(2)\r\n",
    "  \r\n",
    "  doScrollDown(102) # 스크롤다운 \r\n",
    "\r\n",
    "  #02. 크롤링할 개별 포스팅 주소 가져오기 (각 월(3~12)당 포스팅된 컨텐츠 )\r\n",
    "  article_list = driver.find_elements_by_css_selector('.api_txt_lines.total_tit')\r\n",
    "\r\n",
    "  #03. 데이터 수집 시작\r\n",
    "  total_data=[] # 총데이터 저장할 리스트\r\n",
    "\r\n",
    "  # 카페 개별 페이지 접속\r\n",
    "  for article in article_list:\r\n",
    "    try: #제목링크없는 페이지 존재\r\n",
    "      article.click()  # 한페이지에 접속완료\r\n",
    "      time.sleep(5)\r\n",
    "        \r\n",
    "      # 드라이버 윈도우 설정\r\n",
    "      change_tab = driver.window_handles[-1]\r\n",
    "      driver.switch_to.window(change_tab) \r\n",
    "\r\n",
    "      try:\r\n",
    "          data = get_Data(driver) # 수집\r\n",
    "          total_data.append(data)\r\n",
    "      except:\r\n",
    "          pass\r\n",
    "\r\n",
    "      driver.close() # 한페이지 수집완료\r\n",
    "\r\n",
    "      # 다른 게시글 들어갈 준비 (必)\r\n",
    "      change_tab = driver.window_handles[-1]\r\n",
    "      driver.switch_to.window(change_tab)\r\n",
    "    except:\r\n",
    "      # 다른 게시글 들어갈 준비 (必)\r\n",
    "      change_tab = driver.window_handles[-1]\r\n",
    "      driver.switch_to.window(change_tab)\r\n",
    "\r\n",
    "  #04.데이터프레임으로 저장\r\n",
    "  df = pd.DataFrame(total_data,columns=['title','nickname','date','content','comments'])\r\n",
    "\r\n",
    "  #05.엑셀파일로 저장\r\n",
    "  globals()['writer_{}'.format(i)] = pd.ExcelWriter(f'./alldaycrawl/naver_cafe_conents_ssal_2015_{i}월.xlsx')\r\n",
    "  df.to_excel(globals()['writer_{}'.format(i)],index =False)\r\n",
    "  globals()['writer_{}'.format(i)].save()\r\n",
    "\r\n",
    "  ##06. 드라이버 종료\r\n",
    "  driver.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(3,13):  # 3월부터 12월까지\r\n",
    "\r\n",
    "  # 01. 페이지 준비(3월~ 12월 설정)\r\n",
    "  if i == 3:\r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.03.01&date_to=2015.03.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20150301to20150331&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 4:\r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.04.01&date_to=2015.04.30&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20150401to20150430&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 5:\r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.05.01&date_to=2015.05.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20150501to20150531&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 6:\r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.06.01&date_to=2015.06.30&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20150601to20150630&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 7:    \r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.07.01&date_to=2015.07.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20150701to20150731&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 8:    \r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.08.01&date_to=2015.08.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20150801to20150831&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 9:    \r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.09.01&date_to=2015.09.30&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20150901to20150930&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 10:   \r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.10.01&date_to=2015.10.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20151001to20151031&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 11:    \r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.11.01&date_to=2015.11.30&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20151101to20151130&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "  if i == 12:    \r\n",
    "    url = 'https://search.naver.com/search.naver?where=article&query=%EC%8C%80%20-%EC%97%85%EC%B2%B4%2C%20-%EC%A0%9C%EA%B3%B5%EB%B0%9B%2C%20-%EC%A7%80%EC%9B%90%EB%B0%9B%2C%20-%EC%9B%90%EA%B3%A0%EB%A3%8C%2C%20-%EB%AC%B4%EC%83%81%2C%20-%ED%98%91%EC%B0%AC%2C%20-%EC%86%8C%EC%A0%95%EC%9D%98%2C%20-%EC%82%AC%EB%A3%8C%2C%20-%EA%B3%A0%EB%9E%98%EB%B0%A5%2C%20-%EA%B8%B8%EB%83%A5%EC%9D%B4%2C%20-%EA%B3%A0%EC%96%91%EC%9D%B4%2C%20-%EA%B0%95%EC%95%84%EC%A7%80&ie=utf8&st=rel&date_option=99&date_from=2015.12.01&date_to=2015.12.31&board=&srchby=text&dup_remove=1&cafe_url=&without_cafe_url=&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom20151201to20151231&nso_open=1&t=0&mson=0&prdtype=0'\r\n",
    "\r\n",
    "  driver = webdriver.Chrome('./chromedriver') \r\n",
    "  #driver.implicitly_wait(4) \r\n",
    "  driver.get(url)\r\n",
    "  time.sleep(2)\r\n",
    "  \r\n",
    "  doScrollDown(102) # 스크롤다운 \r\n",
    "\r\n",
    "  #02. 크롤링할 개별 포스팅 주소 가져오기 (각 월(3~12)당 포스팅된 컨텐츠 )\r\n",
    "  article_list = driver.find_elements_by_css_selector('.api_txt_lines.total_tit')\r\n",
    "\r\n",
    "  #03. 데이터 수집 시작\r\n",
    "  total_data=[] # 총데이터 저장할 리스트\r\n",
    "\r\n",
    "  # 카페 개별 페이지 접속\r\n",
    "  for article in article_list:\r\n",
    "    try: #제목링크없는 페이지 존재\r\n",
    "      article.click()  # 한페이지에 접속완료\r\n",
    "      time.sleep(5)\r\n",
    "        \r\n",
    "      # 드라이버 윈도우 설정\r\n",
    "      change_tab = driver.window_handles[-1]\r\n",
    "      driver.switch_to.window(change_tab) \r\n",
    "\r\n",
    "      try:\r\n",
    "          data = get_Data(driver) # 수집\r\n",
    "          total_data.append(data)\r\n",
    "      except:\r\n",
    "          pass\r\n",
    "\r\n",
    "      driver.close() # 한페이지 수집완료\r\n",
    "\r\n",
    "      # 다른 게시글 들어갈 준비 (必)\r\n",
    "      change_tab = driver.window_handles[-1]\r\n",
    "      driver.switch_to.window(change_tab)\r\n",
    "    except:\r\n",
    "      # 다른 게시글 들어갈 준비 (必)\r\n",
    "      change_tab = driver.window_handles[-1]\r\n",
    "      driver.switch_to.window(change_tab)\r\n",
    "\r\n",
    "  #04.데이터프레임으로 저장\r\n",
    "  df = pd.DataFrame(total_data,columns=['title','nickname','date','content','comments'])\r\n",
    "\r\n",
    "  #05.엑셀파일로 저장\r\n",
    "  globals()['writer_{}'.format(i)] = pd.ExcelWriter(f'./alldaycrawl/naver_cafe_conents_ssal_2015_{i}월.xlsx')\r\n",
    "  df.to_excel(globals()['writer_{}'.format(i)],index =False)\r\n",
    "  globals()['writer_{}'.format(i)].save()\r\n",
    "\r\n",
    "  ##06. 드라이버 종료\r\n",
    "  driver.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8c1eae21719a0790335dcb83aad72b63b602cfe5cdb2bda0f60bc11d4f154e4b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}